{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c9b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube RAG QA Bot - Complete Implementation with LangSmith\n",
    "# ===========================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import librosa\n",
    "from dotenv import load_dotenv\n",
    "import whisper\n",
    "import gradio as gr\n",
    "import nest_asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from yt_dlp import YoutubeDL \n",
    "\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from pydantic import BaseModel, PrivateAttr\n",
    "\n",
    "# Mistral Integration\n",
    "from mistralai import Mistral, UserMessage\n",
    "\n",
    "# LangSmith Integration\n",
    "from langsmith import Client\n",
    "from langchain.callbacks import LangChainTracer\n",
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac6d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "# ==================== Configuration ====================\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\", \"thisonecanbefinal3\")\n",
    "\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\"‚ùå MISTRAL_API_KEY not found in environment variables\")\n",
    "\n",
    "# LangSmith Configuration\n",
    "if LANGCHAIN_API_KEY:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
    "    print(\"‚úÖ LangSmith tracing enabled\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LANGCHAIN_API_KEY not found - LangSmith tracing disabled\")\n",
    "\n",
    "# Directory Configuration\n",
    "BASE_DIR = \"youtube_rag_project\"\n",
    "AUDIO_DIR = os.path.join(BASE_DIR, \"audio\")\n",
    "TRANSCRIPTS_DIR = os.path.join(BASE_DIR, \"transcripts\")\n",
    "CHROMA_DIR = os.path.join(BASE_DIR, \"chroma_db\")\n",
    "TTS_DIR = os.path.join(BASE_DIR, \"tts\")\n",
    "\n",
    "# Model Configuration\n",
    "WHISPER_MODEL_SIZE = \"base\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "VIDEO_LIMIT = 30\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [AUDIO_DIR, TRANSCRIPTS_DIR, TTS_DIR, CHROMA_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e924872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LangSmith Integration ====================\n",
    "class LangSmithTracker:\n",
    "    \"\"\"LangSmith integration for monitoring and evaluation\"\"\"\n",
    "    \n",
    "def __init__(self):\n",
    "    self.client = None\n",
    "    if LANGCHAIN_API_KEY:\n",
    "        try:\n",
    "            self.client = Client(api_key=LANGCHAIN_API_KEY)\n",
    "            from langsmith.schemas import TracerSessionCreate\n",
    "            \n",
    "            session_name = f\"youtube-rag-session-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "            session = self.client.create_session(\n",
    "                TracerSessionCreate(name=session_name)\n",
    "            )\n",
    "            self.session_id = session.id\n",
    "            \n",
    "            logging.info(f\"‚úÖ LangSmith client initialized, session: {session_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"LangSmith initialization error: {e}\")\n",
    "            self.client = None\n",
    "    \n",
    "def log_interaction(self, question: str, answer: str, sources: List[str] = None):\n",
    "    \"\"\"Log user interactions for evaluation\"\"\"\n",
    "    if not self.client:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        self.client.create_run(\n",
    "            name=\"youtube_rag_query\",\n",
    "            run_type=\"chain\",\n",
    "            inputs={\"question\": question},\n",
    "            outputs={\"answer\": answer, \"sources\": sources or []},\n",
    "            project_name=LANGCHAIN_PROJECT,\n",
    "            session_id=self.session_id,\n",
    "            start_time=datetime.now(),\n",
    "            end_time=datetime.now()\n",
    "        )\n",
    "        logging.info(\"‚úÖ Interaction logged to LangSmith\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LangSmith logging error: {e}\")\n",
    "        \n",
    "    \n",
    "    def create_evaluation_dataset(self, qa_pairs: List[Dict]):\n",
    "        \"\"\"Create evaluation dataset in LangSmith\"\"\"\n",
    "        if not self.client:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            dataset_name = f\"youtube-rag-eval-{datetime.now().strftime('%Y%m%d')}\"\n",
    "            dataset = self.client.create_dataset(\n",
    "                dataset_name=dataset_name,\n",
    "                description=\"YouTube RAG Bot evaluation dataset\"\n",
    "            )\n",
    "            \n",
    "            for qa_pair in qa_pairs:\n",
    "                self.client.create_example(\n",
    "                    dataset_id=dataset.id,\n",
    "                    inputs={\"question\": qa_pair[\"question\"]},\n",
    "                    outputs={\"expected_answer\": qa_pair[\"answer\"]}\n",
    "                )\n",
    "            \n",
    "            logging.info(f\"‚úÖ Created evaluation dataset: {dataset_name}\")\n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Dataset creation error: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed5a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Mistral LLM Wrapper ====================\n",
    "class MistralLLM(LLM, BaseModel):\n",
    "    \"\"\"Custom Mistral LLM wrapper for LangChain\"\"\"\n",
    "    api_key: str\n",
    "    model: str = \"mistral-large-latest\"\n",
    "    temperature: float = 0.0\n",
    "    max_tokens: int = 1000\n",
    "    _mistral_client: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(**data)\n",
    "        self._mistral_client = Mistral(api_key=self.api_key)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mistral\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        try:\n",
    "            messages = [UserMessage(content=prompt)]\n",
    "            response = self._mistral_client.chat.complete(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Mistral API error: {e}\")\n",
    "            return f\"‚ùå Error: Unable to process request\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4540fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Data Processing Pipeline ====================\n",
    "class YouTubeDataProcessor:\n",
    "    \"\"\"Handles YouTube video downloading and transcription\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.whisper_model = None\n",
    "    \n",
    "    def download_videos(self, channel_url: str, limit: int = VIDEO_LIMIT) -> bool:\n",
    "        \"\"\"Download YouTube videos as audio files\"\"\"\n",
    "        existing_files = glob.glob(os.path.join(AUDIO_DIR, \"*.mp3\"))\n",
    "        \n",
    "        if len(existing_files) >= limit:\n",
    "            logging.info(f\"‚úÖ Found {len(existing_files)} audio files, skipping download\")\n",
    "            return True\n",
    "\n",
    "        logging.info(f\"üì• Downloading {limit} videos from channel...\")\n",
    "        \n",
    "        ydl_opts = {\n",
    "            \"format\": \"bestaudio/best\",\n",
    "            \"postprocessors\": [{\n",
    "                \"key\": \"FFmpegExtractAudio\",\n",
    "                \"preferredcodec\": \"mp3\",\n",
    "                \"preferredquality\": \"128\",\n",
    "            }],\n",
    "            \"playlistend\": limit,\n",
    "            \"outtmpl\": os.path.join(AUDIO_DIR, \"%(title)s.%(ext)s\"),\n",
    "            \"ignoreerrors\": True,\n",
    "            \"quiet\": True,\n",
    "            \"no_warnings\": True\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with YoutubeDL(ydl_opts) as ydl:\n",
    "                ydl.download([channel_url])\n",
    "            logging.info(\"‚úÖ Download completed\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Download error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def transcribe_audio_files(self) -> Dict[str, str]:\n",
    "        \"\"\"Transcribe all audio files to text\"\"\"\n",
    "        if self.whisper_model is None:\n",
    "            logging.info(\"üß† Loading Whisper model...\")\n",
    "            self.whisper_model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "        \n",
    "        transcripts = {}\n",
    "        audio_files = glob.glob(os.path.join(AUDIO_DIR, \"*.mp3\"))\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            title = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "            transcript_path = os.path.join(TRANSCRIPTS_DIR, f\"{title}.txt\")\n",
    "            \n",
    "            # Check if transcript already exists\n",
    "            if os.path.exists(transcript_path):\n",
    "                with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    transcripts[title] = f.read()\n",
    "                continue\n",
    "            \n",
    "            logging.info(f\"üéôÔ∏è Transcribing: {title[:50]}...\")\n",
    "            try:\n",
    "                result = self.whisper_model.transcribe(audio_file)\n",
    "                transcript_text = result[\"text\"].strip()\n",
    "                transcripts[title] = transcript_text\n",
    "                \n",
    "                # Save transcript\n",
    "                with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(transcript_text)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Transcription error for {title}: {e}\")\n",
    "        \n",
    "        logging.info(f\"‚úÖ Processed {len(transcripts)} transcripts\")\n",
    "        return transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951bde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RAG Vector Store ====================\n",
    "class RAGVectorStore:\n",
    "    \"\"\"Handles vector database creation and management\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def create_or_load_vectorstore(self, transcripts: Dict[str, str]) -> Chroma:\n",
    "        \"\"\"Create or load existing vector database\"\"\"\n",
    "        \n",
    "        # Check if vectorstore exists\n",
    "        if os.path.exists(os.path.join(CHROMA_DIR, \"chroma.sqlite3\")):\n",
    "            logging.info(\"‚úÖ Loading existing vectorstore...\")\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=CHROMA_DIR, \n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            return self.vectorstore\n",
    "        \n",
    "        logging.info(\"üìö Creating new vectorstore...\")\n",
    "        \n",
    "        # Text splitting and chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "        \n",
    "        documents = []\n",
    "        for title, transcript in transcripts.items():\n",
    "            if not transcript.strip():\n",
    "                continue\n",
    "                \n",
    "            chunks = text_splitter.split_text(transcript)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": title,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"total_chunks\": len(chunks)\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents to vectorize\")\n",
    "        \n",
    "        # Create vectorstore\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=CHROMA_DIR\n",
    "        )\n",
    "        \n",
    "        self.vectorstore.persist()\n",
    "        logging.info(f\"‚úÖ Vectorstore created with {len(documents)} chunks\")\n",
    "        return self.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f029dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Agent Tools ====================\n",
    "def create_vector_search_tool(vectorstore: Chroma) -> Tool:\n",
    "    \"\"\"Create vector search tool for the agent\"\"\"\n",
    "    \n",
    "    def vector_search(query: str) -> str:\n",
    "        \"\"\"Search through YouTube video transcripts\"\"\"\n",
    "        try:\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": 3}\n",
    "            )\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            \n",
    "            if not docs:\n",
    "                return \"‚ùå No relevant information found in the video transcripts.\"\n",
    "            \n",
    "            # Format results\n",
    "            results = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "                content = doc.page_content[:400]  # Limit content\n",
    "                results.append(f\"**Source {i}: {source}**\\n{content}...\")\n",
    "            \n",
    "            return \"\\n\\n\".join(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Vector search error: {e}\")\n",
    "            return f\"‚ùå Search error: {str(e)}\"\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Search through YouTube video transcripts for relevant information. Input should be a search query.\",\n",
    "        func=vector_search\n",
    "    )\n",
    "\n",
    "def create_transcription_tool() -> Tool:\n",
    "    \"\"\"Create audio transcription tool\"\"\"\n",
    "    \n",
    "    def transcribe_audio(audio_path: str) -> str:\n",
    "        \"\"\"Transcribe audio file to text\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(audio_path):\n",
    "                return \"‚ùå Audio file not found\"\n",
    "            \n",
    "            model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "            result = model.transcribe(audio_path)\n",
    "            return f\"üéôÔ∏è Transcription: {result['text']}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return f\"‚ùå Transcription failed: {str(e)}\"\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"transcribe_audio\",\n",
    "        description=\"Transcribe audio input to text. Input should be audio file path.\",\n",
    "        func=transcribe_audio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b55bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Main RAG Agent ====================\n",
    "class YouTubeRAGAgent:\n",
    "    \"\"\"Main RAG agent with tools, memory, and LangSmith tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, mistral_api_key: str, vectorstore: Chroma):\n",
    "        # Initialize LLM\n",
    "        self.llm = MistralLLM(api_key=mistral_api_key)\n",
    "        \n",
    "        # Initialize LangSmith tracker\n",
    "        self.langsmith_tracker = LangSmithTracker()\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferWindowMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            k=6  # Keep last 3 exchanges\n",
    "        )\n",
    "        \n",
    "        # Initialize tools\n",
    "        self.tools = [\n",
    "            create_vector_search_tool(vectorstore),\n",
    "            create_transcription_tool()\n",
    "        ]\n",
    "        \n",
    "        # Create callbacks for LangSmith\n",
    "        callbacks = []\n",
    "        if LANGCHAIN_API_KEY:\n",
    "            callbacks.append(LangChainTracer(project_name=LANGCHAIN_PROJECT))\n",
    "        \n",
    "        # Create agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "            memory=self.memory,\n",
    "            verbose=True, \n",
    "            handle_parsing_errors=True,\n",
    "            max_iterations=6, #increased iterations so to get better reply\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process user question and return answer with metadata\"\"\"\n",
    "        try:\n",
    "            # Enhance prompt for better responses\n",
    "            enhanced_prompt = f\"\"\"\n",
    "            You are a friendly and knowledgeable AI assistant that answers questions based **only** on YouTube video transcripts.\n",
    "\n",
    "            TOOLS:\n",
    "            You have access to the `vector_search` tool to look up relevant information.\n",
    "\n",
    "            RULES:\n",
    "            1.Always use the `vector_search` tool before answering. Never answer from general knowledge.\n",
    "            2.Base your answer **only on the transcript chunks returned** by the tool. If nothing is returned, respond honestly: \"Sorry, I couldn't find any info on that topic in the videos I have.\"\n",
    "            3.Be conversational and clear, like you're explaining to a curious friend.\n",
    "            4.Always include the video title(s) from the source in your response.\n",
    "            5.Never invent video titles or make up facts. If you're unsure, say so.\n",
    "\n",
    "            User Question: {question}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.agent.run(enhanced_prompt)\n",
    "            \n",
    "            # Extract sources from the response (basic implementation)\n",
    "            sources = self._extract_sources(response)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": sources,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Query processing error: {e}\")\n",
    "            error_response = f\"‚ùå Sorry, I encountered an error processing your question: {str(e)}\"\n",
    "            return {\n",
    "                \"answer\": error_response,\n",
    "                \"sources\": [],\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _extract_sources(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract source information from response\"\"\"\n",
    "        sources = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            if \"Source\" in line and \":\" in line:\n",
    "                source = line.split(\":\", 1)[1].strip().replace(\"**\", \"\")\n",
    "                if source and source not in sources:\n",
    "                    sources.append(source)\n",
    "        return sources\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear conversation memory\"\"\"\n",
    "        self.memory.clear()\n",
    "        logging.info(\"üßπ Memory cleared\")\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Get formatted conversation history\"\"\"\n",
    "        history = []\n",
    "        if hasattr(self.memory, 'chat_memory'):\n",
    "            for message in self.memory.chat_memory.messages:\n",
    "                if isinstance(message, HumanMessage):\n",
    "                    history.append({\"role\": \"user\", \"content\": message.content})\n",
    "                elif isinstance(message, AIMessage):\n",
    "                    history.append({\"role\": \"assistant\", \"content\": message.content})\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb8468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Text-to-Speech ====================\n",
    "async def text_to_speech_async(text: str) -> Optional[str]:\n",
    "    \"\"\"Convert text to speech using Edge TTS\"\"\"\n",
    "    try:\n",
    "        import edge_tts\n",
    "        \n",
    "        # Clean text for TTS\n",
    "        clean_text = text.replace(\"**\", \"\").replace(\"*\", \"\")\n",
    "        #chatgpt fix\n",
    "        #if len(clean_text) > 500:\n",
    "        #    clean_text = clean_text[:500] + \"...\"\n",
    "        \n",
    "        output_path = os.path.join(TTS_DIR, \"response.mp3\")\n",
    "        communicate = edge_tts.Communicate(clean_text, voice=\"en-US-JennyNeural\")\n",
    "        await communicate.save(output_path)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"TTS Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def text_to_speech(text: str) -> Optional[str]:\n",
    "    \"\"\"Synchronous wrapper for TTS\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    \n",
    "    return loop.run_until_complete(text_to_speech_async(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce85b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Main Processing Pipeline ====================\n",
    "def process_question(text_input: str, audio_input) -> tuple:\n",
    "    \"\"\"Main processing pipeline for questions\"\"\"\n",
    "    question = \"\"\n",
    "    \n",
    "    # Process audio input first\n",
    "    if audio_input is not None:\n",
    "        try:\n",
    "            # Load and process audio\n",
    "            audio_data, sr = librosa.load(audio_input, sr=16000)\n",
    "            \n",
    "            # Transcribe using Whisper\n",
    "            model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "            result = model.transcribe(audio_data.astype(np.float32))\n",
    "            question = result[\"text\"].strip()\n",
    "            \n",
    "            logging.info(f\"üéôÔ∏è Audio transcribed: {question}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Audio processing error: {e}\")\n",
    "            return f\"‚ùå Audio processing failed: {str(e)}\", None, \"\"\n",
    "    \n",
    "    # Use text input if no audio question\n",
    "    if not question and text_input:\n",
    "        question = text_input.strip()\n",
    "    \n",
    "    # Validate input\n",
    "    if not question:\n",
    "        return \"‚ö†Ô∏è Please provide a question via text or audio.\", None, \"\"\n",
    "    \n",
    "    # Get answer from RAG agent\n",
    "    try:\n",
    "        result = rag_agent.query(question)\n",
    "        answer = result[\"answer\"]\n",
    "        sources = result[\"sources\"]\n",
    "        \n",
    "        # Format sources for display\n",
    "        sources_text = \"\"\n",
    "        if sources:\n",
    "            sources_text = \"\\n\\n**Sources:**\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
    "        \n",
    "        # Generate audio response\n",
    "        audio_path = text_to_speech(answer)\n",
    "        \n",
    "        return answer + sources_text, audio_path, question\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing error: {e}\")\n",
    "        return f\"‚ùå Error processing question: {str(e)}\", None, question\n",
    "\n",
    "def get_conversation_history():\n",
    "    \"\"\"Get formatted conversation history\"\"\"\n",
    "    try:\n",
    "        history = rag_agent.get_conversation_history()\n",
    "        if not history:\n",
    "            return \"No conversation history yet.\"\n",
    "        \n",
    "        formatted_history = []\n",
    "        for entry in history[-6:]:  # Show last 6 entries\n",
    "            role_icon = \"üßë\" if entry[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "            formatted_history.append(f\"{role_icon} **{entry['role'].title()}**: {entry['content']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_history)\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving history: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21d7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Initialize System ====================\n",
    "def initialize_system():\n",
    "    \"\"\"Initialize the complete RAG system\"\"\"\n",
    "    global rag_agent\n",
    "    \n",
    "    logging.info(\"üöÄ Initializing YouTube RAG Bot...\")\n",
    "    \n",
    "    # Initialize data processor\n",
    "    processor = YouTubeDataProcessor()\n",
    "    \n",
    "    # Download videos (Life Noggin channel)\n",
    "    channel_url = \"https://www.youtube.com/@lifenoggin/videos\"\n",
    "    processor.download_videos(channel_url, VIDEO_LIMIT)\n",
    "    \n",
    "    # Transcribe videos\n",
    "    transcripts = processor.transcribe_audio_files()\n",
    "    \n",
    "    if not transcripts:\n",
    "        raise ValueError(\"No transcripts found. Please check video download and transcription.\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store_manager = RAGVectorStore()\n",
    "    vectorstore = vector_store_manager.create_or_load_vectorstore(transcripts)\n",
    "    \n",
    "    # Initialize RAG agent\n",
    "    rag_agent = YouTubeRAGAgent(MISTRAL_API_KEY, vectorstore)\n",
    "    \n",
    "    logging.info(\"‚úÖ System initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a234fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Initialize Gradio ====================\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create the Gradio web interface with LangSmith monitoring and images\"\"\"\n",
    "\n",
    "    with gr.Blocks(\n",
    "        title=\"üß† Blocko Bot The open source alternative of knowledge\", \n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        /* Cyan gradient background */\n",
    "        .gradio-container {\n",
    "            background: linear-gradient(135deg, #00c6ff 0%, #0072ff 100%) !important;\n",
    "            min-height: 100vh;\n",
    "            padding: 40px;\n",
    "            width: 100vw;\n",
    "            max-width: 100% ! important;\n",
    "        }        \n",
    "        .gradio-container {\n",
    "        max-width: 1400px !important;\n",
    "        background-color: rgba(255, 255, 255, 0.85);\n",
    "        padding: 20px;\n",
    "        border-radius: 16px;\n",
    "        }\n",
    "        .langsmith-status {\n",
    "            background: linear-gradient(90deg, #10b981, #059669);\n",
    "            color: white;\n",
    "            padding: 10px;\n",
    "            border-radius: 8px;\n",
    "            margin: 10px 0;\n",
    "            text-align: center;\n",
    "        }\n",
    "        \"\"\"\n",
    "    ) as interface:\n",
    "        \n",
    "        # ‚úÖ Banner Image (Local)\n",
    "        gr.Image(value=\"picture/channels4_banner.jpg\", label=None, interactive=False, show_label=False)\n",
    "\n",
    "\n",
    "\n",
    "        langsmith_status = \"üü¢ LangSmith Tracking: ON\" if LANGCHAIN_API_KEY else \"üü° LangSmith Tracking: OFF\"\n",
    "\n",
    "        gr.Markdown(f\"\"\"\n",
    "        ## üß† YouTube RAG QA Bot with LangChain Agents\n",
    "\n",
    "        <div class=\"langsmith-status\">\n",
    "        {langsmith_status} | Project: {LANGCHAIN_PROJECT}\n",
    "        </div>\n",
    "\n",
    "        This bot allows you to ask questions about YouTube videos using advanced AI techniques:\n",
    "        - ü§ñ **LangChain Agents** with tools & memory\n",
    "        - üìö **Vector Search** via Chroma\n",
    "        - üéôÔ∏è **Voice Input & Output** (Whisper + Edge TTS)\n",
    "        - üìä **LangSmith Monitoring**\n",
    "        \"\"\")\n",
    "\n",
    "        with gr.Tab(\"üí¨ Chat Interface\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    # ‚úÖ Updated Bot Avatar Image Path\n",
    "                    gr.Image(value=\"picture/blocko.png\", label=\"ü§ñ Hi there! I'm Blocko\", interactive=False)\n",
    "\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"üí¨ Ask your question\",\n",
    "                        placeholder=\"e.g., What happens to your body in space?\",\n",
    "                        lines=3\n",
    "                    )\n",
    "\n",
    "                    audio_input = gr.Audio(\n",
    "                        label=\"üéôÔ∏è Or ask with your voice\",\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "\n",
    "                    with gr.Row():\n",
    "                        ask_btn = gr.Button(\"üöÄ Ask Question\", variant=\"primary\", size=\"lg\")\n",
    "                        clear_btn = gr.Button(\"üßπ Clear Memory\", variant=\"secondary\")\n",
    "\n",
    "                with gr.Column(scale=3):\n",
    "                    response_text = gr.Textbox(\n",
    "                        label=\"ü§ñ AI Response\",\n",
    "                        lines=12,\n",
    "                        max_lines=25\n",
    "                    )\n",
    "\n",
    "                    response_audio = gr.Audio(\n",
    "                        label=\"üîä Listen to Response\",\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "\n",
    "                    last_question = gr.Textbox(\n",
    "                        label=\"‚ùì Last Question\",\n",
    "                        lines=1,\n",
    "                        interactive=False\n",
    "                    )\n",
    "\n",
    "        with gr.Tab(\"üìä Conversation History\"):\n",
    "            with gr.Column():\n",
    "                refresh_history_btn = gr.Button(\"üîÑ Refresh History\", variant=\"secondary\")\n",
    "                conversation_history = gr.Textbox(\n",
    "                    label=\"üìú Recent Conversations\",\n",
    "                    lines=15,\n",
    "                    max_lines=25,\n",
    "                    interactive=False\n",
    "                )\n",
    "\n",
    "        with gr.Tab(\"‚öôÔ∏è System Info\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## üß© System Architecture\n",
    "\n",
    "\n",
    "            ### Configuration\n",
    "            - **Model**: Mistral Large Latest\n",
    "            - **Embedding**: all-MiniLM-L6-v2\n",
    "            - **Whisper**: base\n",
    "            - **Vector DB**: ChromaDB\n",
    "            - **LangSmith Project**: final3\n",
    "            \"\"\")\n",
    "\n",
    "        # Event handlers\n",
    "        ask_btn.click(\n",
    "            fn=process_question,\n",
    "            inputs=[text_input, audio_input],\n",
    "            outputs=[response_text, response_audio, last_question]\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=lambda: (rag_agent.clear_memory(), \"Memory cleared! üßπ\"),\n",
    "            inputs=[],\n",
    "            outputs=[conversation_history]\n",
    "        )\n",
    "\n",
    "        refresh_history_btn.click(\n",
    "            fn=get_conversation_history,\n",
    "            inputs=[],\n",
    "            outputs=[conversation_history]\n",
    "        )\n",
    "\n",
    "        # Example questions\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Is AI dangerous?\"],\n",
    "                [\"How much caffeine per day is recommended?\"],\n",
    "                [\"What is Stress?\"]\n",
    "            ],\n",
    "            inputs=text_input\n",
    "        )\n",
    "\n",
    "    return interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89bd7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üöÄ Initializing YouTube RAG Bot...\n",
      "INFO:root:‚úÖ Found 30 audio files, skipping download\n",
      "INFO:root:üß† Loading Whisper model...\n",
      "INFO:root:‚úÖ Processed 30 transcripts\n",
      "WARNING:tensorflow:From C:\\Users\\rospi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO:datasets:PyTorch version 2.7.1 available.\n",
      "INFO:datasets:TensorFlow version 2.19.0 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:root:‚úÖ Loading existing vectorstore...\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:root:‚úÖ System initialized successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:root:üéôÔ∏è Audio transcribed: How many hours per day should I sleep?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: How many hours per day should I sleep?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Make Your Mornings Not Suck**\n",
      ". In addition to being good for your health, a good night's rest is good for your mood and can also reduce your stress level. Adults 18 and over need at least 7 hours of sleep each night, while teens need 8 to 10 hours. People also make sure the next step is much easier, which is don't hit the snooze button. Set your alarm so you have enough time to do your morning routine and when it goes off, ge...\n",
      "\n",
      "**Source 2: What Happens When You Go To Sleep SadÔºü**\n",
      ". Could be the dragonfly thing. Who knows. You can see how this can really mess up your sleep quality, which will make you feel even worse the next day. In addition to affecting your cognitive abilities, like your memory and attention, studies have found that poor sleep, including staying up late or sleeping less than usual, leads to poor mental health. That's because less sleep means less REM sle...\n",
      "\n",
      "**Source 3: What Happens When You Go To Sleep SadÔºü**\n",
      ". People who suffer from insomnia are five times more likely to develop depression and 20 times more likely to develop panic disorder, which is a type of anxiety disorder. Chronic poor sleep can also have lasting effects on other parts of your body. It could increase the likelihood of developing dementia, heart disease, type 2 diabetes, obesity, and certain cancers, such as breast, colon, ovarian,...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üßπ Memory cleared\n",
      "INFO:root:üéôÔ∏è Audio transcribed: How many hours per day should I sleep?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: How many hours per day should I sleep?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Make Your Mornings Not Suck**\n",
      ". In addition to being good for your health, a good night's rest is good for your mood and can also reduce your stress level. Adults 18 and over need at least 7 hours of sleep each night, while teens need 8 to 10 hours. People also make sure the next step is much easier, which is don't hit the snooze button. Set your alarm so you have enough time to do your morning routine and when it goes off, ge...\n",
      "\n",
      "**Source 2: What Happens When You Go To Sleep SadÔºü**\n",
      ". Could be the dragonfly thing. Who knows. You can see how this can really mess up your sleep quality, which will make you feel even worse the next day. In addition to affecting your cognitive abilities, like your memory and attention, studies have found that poor sleep, including staying up late or sleeping less than usual, leads to poor mental health. That's because less sleep means less REM sle...\n",
      "\n",
      "**Source 3: What Happens When You Go To Sleep SadÔºü**\n",
      ". People who suffer from insomnia are five times more likely to develop depression and 20 times more likely to develop panic disorder, which is a type of anxiety disorder. Chronic poor sleep can also have lasting effects on other parts of your body. It could increase the likelihood of developing dementia, heart disease, type 2 diabetes, obesity, and certain cancers, such as breast, colon, ovarian,...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "\n",
      "AI: Based on the information from the video transcripts, adults aged 18 and over need at least 7 hours of sleep each night, while teens need 8 to 10 hours. This information is from the video titled \"Make Your Mornings Not Suck.\" It's important to note that poor sleep can have significant effects on your health, including increased risks for various diseases and mental health issues, as mentioned in the video \"What Happens When You Go To Sleep SadÔºü\". So, make sure you're getting enough rest!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üéôÔ∏è Audio transcribed: Who is the president of the United States of America?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: Who is the president of the United States of America?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Your Brain Is Broken‚ÄîHere's Why**\n",
      ". This phenomenon is known as an attention blindness, where you don't see things that are right in front of you simply because you're paying attention to something else. This may be because you're completing a certain task, like counting passes or footballs, or because something is distracting you, like a bright color, movement, or a familiar face. But this doesn't only happen when there's a lot o...\n",
      "\n",
      "**Source 2: üòÆüëâ Life Noggin Has a BIG Ask!**\n",
      ". Thank you for supporting us and for being curious with us. We'll see you on the Patreon or on our next video, but as always, my name is Blocko. This has been Life Noggin. I think I gave Pat a concussion and don't forget to keep on thinking....\n",
      "\n",
      "**Source 3: Can This Drug Make You ThinÔºü üèÉ‚Äç‚ôÇÔ∏èüíä**\n",
      ". I'm not going to ask. I'm just removing his phone privileges. He is banned from buying anything on Instagram and TikTok shop. Want to watch even more Life Noggin? Click here to watch this video, or here to watch this video YouTube thinks you'll like. As always, my name is Blocko. This has been Life Noggin. Don't forget to keep on thinking....\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Sorry, I couldn't find any info on that topic in the videos I have.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üéôÔ∏è Audio transcribed: It's a dangerous.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üéôÔ∏è Audio transcribed: It's a dangerous.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: It's a dangerous\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Would You Try Any of the Most Dangerous JobsÔºü üíÄ**\n",
      ". Number three is Construction and Extraction. Those working construction and extraction jobs had a fatality rate of 13 in 2022, with 77% of these deaths suffered by construction workers. The construction industry encompasses a lot of different occupations. Some of the most dangerous jobs in this category include structural iron and steel workers, those who construct and install the framework of b...\n",
      "\n",
      "**Source 2: What Happens to Your Body RIGHT AFTER You Get an InfectionÔºü**\n",
      "In this moment, something just slipped into your body. And no, not like that. Stop giggling. You don't know it. You can't feel it, but the infection has already begun. So what happens next when you- Sorry, I can't keep doing this voice. This video is about what happens to your body after you get an infection. Hey there! Welcome to Life Nuggin. As the name suggests, microbes are microscopic. Invisi...\n",
      "\n",
      "**Source 3: Do You Enjoy The Bugs In Your ChocolateÔºü üêúüç´**\n",
      ". Gross. Luckily, like rodent hair, the FDA requires there to be less than one rodent pellet per subsample. So you can enjoy your popcorn without fear. Wanna watch even more Life Noggin? Click here to watch this video, or here to watch this video YouTube thinks you'll like. As always, my name is Blocko. This has been Life Noggin. Don't forget to keep on thinking....\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üéôÔ∏è Audio transcribed: irtiyda ƒ∞atever\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: irtiyda ƒ∞atever\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Why Is Space Dark if the Sun Is So BrightÔºü üåå**\n",
      "Dear Blocko, why is space dark if the sun is so bright? Yeah, if the sun shines at a whopping 3,860 trillion trillion watts, then why is space so dark? Good question. Space is huge. No one knows how huge, but scientists estimate that the observable universe, just the part we can see, is 46 billion light-years in every direction, creating a sphere with a volume of 410 non-nillion cubic light-years....\n",
      "\n",
      "**Source 2: üòÆüëâ Life Noggin Has a BIG Ask!**\n",
      ". The entire Life Noggin team, Greek included, is really passionate about trying to continue our journey. And that's why we're launching the Patreon. We have one tier right now to make it easy. $3 gets you your name at the end of every video, exclusive merch discounts, and one team chat a month doing everything from brainstorming video ideas, drawing really terrible sketches of future thumbnails, ...\n",
      "\n",
      "**Source 3: Is Too Much Screen Time Ruining Your BodyÔºü üò®**\n",
      ". Your body and future self will thank you. Want to watch even more Life Noggin? Click here to watch this video, or here to watch this video YouTube thinks you'll like. As always, my name is Blocko. This has been Life Noggin. Don't forget to keep on thinking....\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Sorry, I couldn't find any relevant information on \"irtiyda ƒ∞atever\" in the videos I have. Could you please provide more context or clarify your question?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: is ai dangerous?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Are You SURE You're Ready for the AI RoommateÔºü**\n",
      ". Not to mention the question of whether or not intelligent machines should be granted certain rights. That's a whole different story for another video. With technology advancing as fast as it's been, we better be ready for it. I mean, we probably won't, just looking at history. But we better, because this guy might be your roommate in your lifetime. So how do you feel about AI robot technology? D...\n",
      "\n",
      "**Source 2: Are You SURE You're Ready for the AI RoommateÔºü**\n",
      ". Tesla promises a humanoid robot capable of doing basically anything you want. Supposedly, one day we'll be able to babysit the kids, walk the dog, get the groceries, and be your friend. If you combine all that with AI's problem solving and decision-making abilities, you'll never need to lift a finger. But integrating this type of technology into your home could have consequences apart from being...\n",
      "\n",
      "**Source 3: Are You SURE You're Ready for the AI RoommateÔºü**\n",
      ". AI robots have already transformed your world and how you do things. Engineers have developed smart machines that save you time and energy by doing mundane tasks. The most well-known bots for homes are cleaning robots that vacuum floors, wash windows, and scrub pools. Gardening robots that weed and mow, smart thermostats that learn your temperature preferences and schedules, and smart security s...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Based on the information from the video transcripts, AI technology is advancing rapidly and could have significant impacts on our daily lives. For instance, AI robots are being developed to handle a wide range of tasks, from babysitting and walking the dog to cleaning and maintaining homes. This information is from the video titled \"Are You SURE You're Ready for the AI RoommateÔºü\".\n",
      "\n",
      "However, integrating such advanced technology into our homes could have consequences beyond just convenience. There are ethical considerations, such as whether intelligent machines should be granted certain rights, and practical concerns about how these technologies will affect our daily routines and interactions.\n",
      "\n",
      "So, while AI has the potential to make our lives easier, it also raises important questions and challenges that we need to be prepared for.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: is ai my roommate?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Are You SURE You're Ready for the AI RoommateÔºü**\n",
      ". AI robots have already transformed your world and how you do things. Engineers have developed smart machines that save you time and energy by doing mundane tasks. The most well-known bots for homes are cleaning robots that vacuum floors, wash windows, and scrub pools. Gardening robots that weed and mow, smart thermostats that learn your temperature preferences and schedules, and smart security s...\n",
      "\n",
      "**Source 2: Are You SURE You're Ready for the AI RoommateÔºü**\n",
      ". Not to mention the question of whether or not intelligent machines should be granted certain rights. That's a whole different story for another video. With technology advancing as fast as it's been, we better be ready for it. I mean, we probably won't, just looking at history. But we better, because this guy might be your roommate in your lifetime. So how do you feel about AI robot technology? D...\n",
      "\n",
      "**Source 3: Are You SURE You're Ready for the AI RoommateÔºü**\n",
      ". Tesla promises a humanoid robot capable of doing basically anything you want. Supposedly, one day we'll be able to babysit the kids, walk the dog, get the groceries, and be your friend. If you combine all that with AI's problem solving and decision-making abilities, you'll never need to lift a finger. But integrating this type of technology into your home could have consequences apart from being...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `‚ùå Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "\n",
      "AI: Based on the information from the video transcripts, AI technology is advancing rapidly, and it's possible that AI robots could become a part of our daily lives, even as roommates. For example, Tesla is developing a humanoid robot that could handle various tasks like babysitting, walking the dog, getting groceries, and even being a friend. This information is from the video titled \"Are You SURE You're Ready for the AI RoommateÔºü\".\n",
      "\n",
      "However, integrating such advanced technology into our homes raises important questions and challenges. There are ethical considerations, such as whether intelligent machines should be granted certain rights, and practical concerns about how these technologies will affect our daily routines and interactions.\n",
      "\n",
      "So, while AI has the potential to make our lives easier, it also brings up significant issues that we need to be prepared for.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# ==================== Main Execution ====================\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the system\n",
    "        initialize_system()\n",
    "        \n",
    "        # Create and launch interface\n",
    "        app = create_gradio_interface()\n",
    "        app.launch(\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7860,\n",
    "            share=False,  # Creates public link\n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to start application: {e}\")\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1345e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
